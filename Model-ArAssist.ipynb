{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c49ea93-52f6-4897-95f4-6d11a69e1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pyttsx3\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839415ed-a4b5-4616-82d1-7731417d4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Camera Field of View\n",
    "CAMERA_FOV = 90  \n",
    "PROCESS_INTERVAL = 0.25  # Process frames every 0.25 seconds\n",
    "\n",
    "# Object Tracking\n",
    "tracked_objects = {}  # {ID: (class_name, x_center, y_center, depth)}\n",
    "spoken_objects = {}  # {class_name: (last_spoken_time, min_distance, direction)}\n",
    "speech_queue = queue.Queue()\n",
    "\n",
    "# Lock for speech thread\n",
    "speech_lock = threading.Lock()\n",
    "\n",
    "def load_depth_model(device):\n",
    "    \"\"\"Load MiDaS depth estimation model.\"\"\"\n",
    "    depth_model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\").to(device)\n",
    "    depth_model.eval()\n",
    "    transform = Compose([ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    return depth_model, transform\n",
    "\n",
    "def load_dino_model(device):\n",
    "    \"\"\"Load DINO model.\"\"\"\n",
    "    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "    dino_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "    return processor, dino_model\n",
    "\n",
    "def speech_worker():\n",
    "    \"\"\"Thread worker that speaks detected objects concisely.\"\"\"\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty(\"rate\", 150)\n",
    "    engine.setProperty(\"volume\", 1.0)\n",
    "\n",
    "    while True:\n",
    "        objects_to_speak = speech_queue.get()\n",
    "        if objects_to_speak == \"STOP\":\n",
    "            engine.stop()\n",
    "            break\n",
    "        \n",
    "        with speech_lock:\n",
    "            sentence = \". \".join(objects_to_speak) + \".\"\n",
    "            engine.say(sentence)\n",
    "            engine.runAndWait()\n",
    "        \n",
    "        speech_queue.task_done()\n",
    "\n",
    "speech_thread = threading.Thread(target=speech_worker, daemon=True)\n",
    "speech_thread.start()\n",
    "\n",
    "def speak(detected_objects):\n",
    "    \"\"\"Announce each object type only once with the closest detected instance.\"\"\"\n",
    "    current_time = time.time()\n",
    "    speech_items = []\n",
    "    \n",
    "    for class_name, (last_time, min_distance, direction) in detected_objects.items():\n",
    "        if class_name not in spoken_objects or (current_time - spoken_objects[class_name][0] > 5):\n",
    "            speech_items.append(f\"{class_name} at {min_distance:.2f} meters, {direction}\")\n",
    "            spoken_objects[class_name] = (current_time, min_distance, direction)\n",
    "    \n",
    "    if speech_items:\n",
    "        speech_queue.put(speech_items)\n",
    "\n",
    "def estimate_depth(frame, depth_model, transform, device):\n",
    "    \"\"\"Estimate depth using MiDaS model.\"\"\"\n",
    "    frame = cv2.resize(frame, (frame.shape[1] // 32 * 32, frame.shape[0] // 32 * 32))\n",
    "    frame_tensor = transform(frame).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth_map = depth_model(frame_tensor)\n",
    "\n",
    "    depth_map = depth_map.squeeze().cpu().numpy()\n",
    "    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())  \n",
    "    depth_map = cv2.resize(depth_map, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    return depth_map\n",
    "\n",
    "def calculate_angle(frame_width, x_center):\n",
    "    \"\"\"Calculate object direction based on x-center position.\"\"\"\n",
    "    relative_position = (x_center - frame_width / 2) / (frame_width / 2)\n",
    "    angle = relative_position * (CAMERA_FOV / 2)\n",
    "    return angle\n",
    "\n",
    "def describe_direction(angle):\n",
    "    \"\"\"Convert angle into human-friendly direction.\"\"\"\n",
    "    if angle < -30:\n",
    "        return \"far left\"\n",
    "    elif angle < -15:\n",
    "        return \"left\"\n",
    "    elif angle < 15:\n",
    "        return \"center\"\n",
    "    elif angle < 30:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"far right\"\n",
    "\n",
    "def detect_objects(frame, processor, dino_model, depth_map, device):\n",
    "    \"\"\"Detect and summarize objects without repetition.\"\"\"\n",
    "    image = Image.fromarray(frame)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = dino_model(**inputs)\n",
    "\n",
    "    logits = outputs.logits.softmax(-1)[0]  \n",
    "    boxes = outputs.pred_boxes[0]  \n",
    "    \n",
    "    detected_objects = {}\n",
    "    id2label = dino_model.config.id2label  \n",
    "\n",
    "    for i in range(logits.shape[0]):  \n",
    "        class_id = logits[i].argmax().item()\n",
    "        class_score = logits[i].max().item()\n",
    "        if class_score < 0.7:  \n",
    "            continue\n",
    "\n",
    "        class_name = id2label.get(class_id, \"Unknown Object\")  \n",
    "        x_center, y_center, w, h = boxes[i].detach().cpu().numpy()\n",
    "        x_center, y_center = x_center * frame.shape[1], y_center * frame.shape[0]\n",
    "\n",
    "        depth = depth_map[int(y_center), int(x_center)] if 0 <= y_center < depth_map.shape[0] and 0 <= x_center < depth_map.shape[1] else float('nan')\n",
    "        angle = calculate_angle(frame.shape[1], x_center)\n",
    "        direction = describe_direction(angle)\n",
    "\n",
    "        if class_name not in detected_objects or depth < detected_objects[class_name][1]:\n",
    "            detected_objects[class_name] = (time.time(), depth, direction)\n",
    "    \n",
    "    speak(detected_objects)\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ðŸŽ¯ Using device: {device}\")\n",
    "\n",
    "    processor, dino_model = load_dino_model(device)\n",
    "    depth_model, transform = load_depth_model(device)\n",
    "\n",
    "    cap = cv2.VideoCapture(\"AddVideoFilePath\")\n",
    "    if not cap.isOpened():\n",
    "        return\n",
    "\n",
    "    last_process_time = 0  \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        current_time = time.time()\n",
    "        if current_time - last_process_time >= PROCESS_INTERVAL:  \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            depth_map = estimate_depth(frame_rgb, depth_model, transform, device)\n",
    "            detect_objects(frame_rgb, processor, dino_model, depth_map, device)\n",
    "            last_process_time = current_time  \n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    speech_queue.put(\"STOP\")  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da42bac-a58d-4289-a568-74154c20bea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
